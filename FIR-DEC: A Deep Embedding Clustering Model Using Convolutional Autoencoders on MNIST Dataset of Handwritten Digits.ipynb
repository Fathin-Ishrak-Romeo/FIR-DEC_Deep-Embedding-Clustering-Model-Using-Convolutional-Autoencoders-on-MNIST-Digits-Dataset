{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay0UtCVhJCWB"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "hM17IQ45JKQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "train_ds = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "hQoU-vn2JO5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim=10):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Conv2d(1,32,3,padding=1), nn.ReLU(True), nn.BatchNorm2d(32), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1), nn.ReLU(True), nn.BatchNorm2d(64), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_enc = nn.Linear(64*7*7, latent_dim)\n",
        "        self.fc_dec = nn.Linear(latent_dim, 64*7*7)\n",
        "        # Decoder\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64,32,3,stride=2,padding=1,output_padding=1),\n",
        "              nn.ReLU(True), nn.BatchNorm2d(32),\n",
        "            nn.ConvTranspose2d(32,1,3,stride=2,padding=1,output_padding=1),\n",
        "              nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.enc(x)                      # → (B,64,7,7)\n",
        "        x = x.view(x.size(0), -1)            # → (B,64*7*7)\n",
        "        z = self.fc_enc(x)                   # → (B,latent_dim)\n",
        "        x = self.fc_dec(z).view(-1,64,7,7)   # → (B,64,7,7)\n",
        "        out = self.dec(x)                    # → (B,1,28,28)\n",
        "        return out, z\n",
        "\n",
        "# Instantiate & count params\n",
        "latent_dim = 10\n",
        "ae = ConvAutoencoder(latent_dim).to(device)\n",
        "print(\"Trainable params:\", sum(p.numel() for p in ae.parameters() if p.requires_grad))\n",
        "\n",
        "# Pre‐train AE\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(ae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "epochs_ae = 20\n",
        "\n",
        "for ep in range(epochs_ae):\n",
        "    ae.train()\n",
        "    total_loss = 0\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recons, _ = ae(imgs)\n",
        "        loss = criterion(recons, imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()*imgs.size(0)\n",
        "    print(f\"AE Epoch {ep+1}/{epochs_ae} — Loss: {total_loss/len(train_ds):.4f}\")"
      ],
      "metadata": {
        "id": "Ipepp2PgJRfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract latent codes for all training data\n",
        "ae.eval()\n",
        "all_z = []\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        _, z = ae(imgs)\n",
        "        all_z.append(z.cpu().numpy())\n",
        "all_z = np.vstack(all_z)  # shape (60000, latent_dim)\n",
        "\n",
        "# K-Means to get initial μ\n",
        "n_clusters = 10\n",
        "km = KMeans(n_clusters=n_clusters, random_state=0).fit(all_z)\n",
        "mu = torch.tensor(km.cluster_centers_, dtype=torch.float, device=device)  # (10,latent_dim)"
      ],
      "metadata": {
        "id": "7E-CK9_nJUIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DEC(nn.Module):\n",
        "    def __init__(self, conv_encoder, fc_enc, mu, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.conv_encoder = conv_encoder\n",
        "        self.fc_enc = fc_enc\n",
        "        self.mu = nn.Parameter(mu)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv_encoder(x)                    # (B,64,7,7)\n",
        "        h = h.view(h.size(0), -1)                   # (B,64*7*7)\n",
        "        z = self.fc_enc(h)                          # (B,latent_dim)\n",
        "        # compute q exactly as before...\n",
        "        dist = torch.sum((z.unsqueeze(1) - self.mu.unsqueeze(0))**2, dim=2)\n",
        "        q = (1.0 + dist / self.alpha)**(- (self.alpha+1)/2)\n",
        "        q = (q.t() / torch.sum(q, dim=1)).t()\n",
        "        return z, q\n",
        "\n",
        "# Target distribution P\n",
        "def target_dist(q):\n",
        "    weight = q**2 / torch.sum(q, dim=0, keepdim=True)\n",
        "    return (weight.t() / torch.sum(weight, dim=1)).t()\n",
        "\n",
        "dec = DEC(ae.enc, ae.fc_enc, mu).to(device)\n",
        "optimizer_dec = optim.Adam(dec.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Eb77omXGJZQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_dec = 30\n",
        "tol = 0.001\n",
        "prev_q = None\n",
        "\n",
        "for ep in range(epochs_dec):\n",
        "    dec.train()\n",
        "    total_loss = 0\n",
        "    # Accumulate Q over all data for convergence check\n",
        "    all_q = []\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        _, q = dec(imgs)\n",
        "        p = target_dist(q).detach()\n",
        "        loss = torch.nn.functional.kl_div(q.log(), p, reduction='batchmean')\n",
        "        optimizer_dec.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_dec.step()\n",
        "        total_loss += loss.item()*imgs.size(0)\n",
        "        all_q.append(q.cpu().detach().numpy())\n",
        "    all_q = np.vstack(all_q)\n",
        "    # Check change in assignments\n",
        "    if prev_q is not None:\n",
        "        diff = np.linalg.norm(all_q - prev_q) / all_q.shape[0]\n",
        "        if diff < tol:\n",
        "            print(f\"Converged at epoch {ep+1}\")\n",
        "            break\n",
        "    prev_q = all_q\n",
        "    print(f\"DEC Epoch {ep+1}/{epochs_dec} — KL Loss: {total_loss/len(train_ds):.4f}\")"
      ],
      "metadata": {
        "id": "YHk0Q_ZAJdAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract final embeddings & hard assignments\n",
        "dec.eval()\n",
        "embeds, qs = [], []\n",
        "labels_true = []\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        z, q = dec(imgs)\n",
        "        embeds.append(z.cpu().numpy())\n",
        "        qs.append(q.cpu().numpy())\n",
        "        labels_true.append(lbls.numpy())\n",
        "embeds = np.vstack(embeds)\n",
        "qs     = np.vstack(qs)\n",
        "labels_true = np.concatenate(labels_true)\n",
        "\n",
        "# Hard cluster = argmax_j q_ij\n",
        "labels_dec = np.argmax(qs, axis=1)\n",
        "\n",
        "# Metrics\n",
        "sil = silhouette_score(embeds, labels_dec)\n",
        "db  = davies_bouldin_score(embeds, labels_dec)\n",
        "ari = adjusted_rand_score(labels_true, labels_dec)\n",
        "print(f\"DEC Clustering — Silhouette: {sil:.3f}, DBI: {db:.3f}, ARI: {ari:.3f}\")\n",
        "\n",
        "# t-SNE plot\n",
        "tsne = TSNE(n_components=2, random_state=0, perplexity=30)\n",
        "emb2d = tsne.fit_transform(embeds)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sc = plt.scatter(emb2d[:,0], emb2d[:,1], c=labels_dec, cmap='tab10', s=10)\n",
        "plt.colorbar(sc, ticks=range(n_clusters))\n",
        "plt.title('t-SNE of DEC Embeddings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AGjmi-mSJfyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
